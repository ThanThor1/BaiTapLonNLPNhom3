{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaH100","dataSources":[{"sourceId":118448,"databundleVersionId":14559231,"sourceType":"competition"},{"sourceId":14167693,"sourceType":"datasetVersion","datasetId":9030808,"isSourceIdPinned":false},{"sourceId":14181172,"sourceType":"datasetVersion","datasetId":9040536,"isSourceIdPinned":false},{"sourceId":14186630,"sourceType":"datasetVersion","datasetId":9044986},{"sourceId":14188617,"sourceType":"datasetVersion","datasetId":9046552}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n!pip install -q -U \"protobuf==3.20.3\" \"transformers>=4.51.0\" datasets accelerate peft trl wandb sacrebleu","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T09:53:52.630438Z","iopub.execute_input":"2025-12-17T09:53:52.630900Z","iopub.status.idle":"2025-12-17T09:53:55.901730Z","shell.execute_reply.started":"2025-12-17T09:53:52.630873Z","shell.execute_reply":"2025-12-17T09:53:55.901083Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import os\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom peft import PeftModel\nfrom tqdm import tqdm\nfrom pathlib import Path\n\nBASE_MODEL = \"Qwen/Qwen3-1.7B\"\nCKPT_DIR = Path(\"/kaggle/input/stage-2/checkpoint-774\")\nMERGED_DIR = \"Qwen3-1.7B-medical\"\n\nassert os.path.isdir(CKPT_DIR), f\"Không thấy checkpoint dir: {CKPT_DIR}\"\ntokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True)\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\n\nbase = AutoModelForCausalLM.from_pretrained(\n    BASE_MODEL,\n    device_map=\"auto\",\n    dtype=torch.bfloat16,\n    attn_implementation=\"sdpa\",\n)\n\npeft_model = PeftModel.from_pretrained(base, CKPT_DIR)\nmerged = peft_model.merge_and_unload()\n\nmerged.config.use_cache = True\nmerged.eval()\n\nmerged.save_pretrained(MERGED_DIR, safe_serialization=True)\ntokenizer.save_pretrained(MERGED_DIR)\n\nprint(\"Saved merged model to:\", MERGED_DIR)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T09:53:55.902666Z","iopub.execute_input":"2025-12-17T09:53:55.902824Z","iopub.status.idle":"2025-12-17T09:54:12.097305Z","shell.execute_reply.started":"2025-12-17T09:53:55.902805Z","shell.execute_reply":"2025-12-17T09:54:12.096781Z"}},"outputs":[{"name":"stderr","text":"2025-12-17 09:53:59.489637: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1765965239.505158     399 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1765965239.509794     399 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ebd2d774a5d44bb9fad736ed972a05b"}},"metadata":{}},{"name":"stdout","text":"Saved merged model to: Qwen3-1.7B-medical\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import os\nimport wandb\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\nos.environ[\"WANDB_PROJECT\"] = \"Qwen3-1.7B-LoRA\"\nos.environ[\"WANDB_LOG_MODEL\"] = \"false\"\n\nwandb.login(key=\"17cf64fcdf2e849c5b569d29066ba2193798ba02\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T09:54:12.097880Z","iopub.execute_input":"2025-12-17T09:54:12.098303Z","iopub.status.idle":"2025-12-17T09:54:19.984873Z","shell.execute_reply.started":"2025-12-17T09:54:12.098289Z","shell.execute_reply":"2025-12-17T09:54:19.984438Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmightdung7105\u001b[0m (\u001b[33mmightdung7105-vietnam-national-university-hanoi\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"run = wandb.init(\n    project=os.environ[\"WANDB_PROJECT\"],\n    name=\"Qwen3-1.7B-LoRA\",\n    notes=\"LoRA SFT VI->EN\",\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T09:54:19.985826Z","iopub.execute_input":"2025-12-17T09:54:19.986211Z","iopub.status.idle":"2025-12-17T09:54:26.524488Z","shell.execute_reply.started":"2025-12-17T09:54:19.986198Z","shell.execute_reply":"2025-12-17T09:54:26.524042Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.23.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20251217_095420-y2cvj9i1</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/mightdung7105-vietnam-national-university-hanoi/Qwen3-1.7B-LoRA/runs/y2cvj9i1' target=\"_blank\">Qwen3-1.7B-LoRA</a></strong> to <a href='https://wandb.ai/mightdung7105-vietnam-national-university-hanoi/Qwen3-1.7B-LoRA' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/mightdung7105-vietnam-national-university-hanoi/Qwen3-1.7B-LoRA' target=\"_blank\">https://wandb.ai/mightdung7105-vietnam-national-university-hanoi/Qwen3-1.7B-LoRA</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/mightdung7105-vietnam-national-university-hanoi/Qwen3-1.7B-LoRA/runs/y2cvj9i1' target=\"_blank\">https://wandb.ai/mightdung7105-vietnam-national-university-hanoi/Qwen3-1.7B-LoRA/runs/y2cvj9i1</a>"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"import unicodedata\nfrom datasets import Dataset, concatenate_datasets # Import thêm concatenate_datasets\n\n# (Giữ nguyên các hàm clean_line, read_lines, group_lines_into_segments, build_segment_dataset)\n# ... [VUI LÒNG GIỮ NGUYÊN HOẶC CHẠY LẠI ĐỊNH NGHĨA CÁC HÀM NÀY TỪ CÁC BƯỚC TRƯỚC] ...\ndef clean_line(s: str) -> str:\n    if s is None:\n        return \"\"\n    s = s.replace(\"\\ufeff\", \"\")\n    s = unicodedata.normalize(\"NFKC\", s)\n    s = s.replace(\"\\u2013\", \"-\").replace(\"\\u2014\", \"-\")\n    s = s.replace('\\\\u200b', '').replace(\"\\u200e\", \"\")\n    s = s.replace(\"“\", '\\\"').replace(\"”\", '\\\"')\n    s = s.replace(\"‘\", \"'\").replace(\"’\", \"'\")\n    s = s.replace(\"‟\", '\\\"')\n    s = s.replace(\"‛\", \"'\")\n    s = \" \".join(s.strip().split())\n    return s\n\ndef read_lines(path: str):\n    with open(path, \"r\", encoding=\"utf-8\") as f:\n        return [clean_line(line.rstrip(\"\\n\")) for line in f]\n\ndef group_lines_into_segments(lines: list[str], N: int = 10) -> list[str]:\n    segments = []\n    for i in range(0, len(lines), N):\n        chunk = lines[i:i + N]\n        segments.append(\"\\n\".join(chunk))\n    return segments\n\ndef build_segment_dataset(en_path: str, vi_path: str, N_lines: int = 10):\n    en_lines = read_lines(en_path)\n    vi_lines = read_lines(vi_path)\n    \n    if len(en_lines) != len(vi_lines):\n        raise ValueError(f\"Line count mismatch: en={len(en_lines)} vi={len(vi_lines)}\")\n        \n    total_lines = len(en_lines)\n    aligned_total = (total_lines // N_lines) * N_lines\n    \n    en_segments = group_lines_into_segments(en_lines[:aligned_total], N=N_lines)\n    vi_segments = group_lines_into_segments(vi_lines[:aligned_total], N=N_lines)\n    \n    dropped_truncation = total_lines - aligned_total \n\n    rows = []\n    dropped_empty = 0\n    \n    for i, (src, tgt) in enumerate(zip(en_segments, vi_segments)):\n        if not src.strip() or not tgt.strip():\n            dropped_empty += 1\n            continue\n        rows.append({\"en\": src, \"vi\": tgt, \"idx\": i * N_lines}) \n\n    ds = Dataset.from_list(rows)\n    return ds, dropped_empty * N_lines + dropped_truncation\n# -----------------------------------------------------------------------------\n\n\n# 1. Tải và tiền xử lý ngữ liệu GỐC (Train/Validation Split Source)\nraw_ds_original, dropped_orig = build_segment_dataset(\n    \"/kaggle/input/train-data/train.en.txt\", \n    \"/kaggle/input/train-data/train.vi.txt\", \n    N_lines=10\n)\nprint(\"Total Original segments:\", len(raw_ds_original), \"Dropped lines:\", dropped_orig)\n\nsplit = raw_ds_original.train_test_split(test_size=0.01, seed=42)\ntrain_ds_orig = split[\"train\"]\neval_ds = split[\"test\"]\n\ntry:\n    raw_ds_add, dropped_add = build_segment_dataset(\n        \"/kaggle/input/train-add/train_en_add.txt\", \n        \"/kaggle/input/train-data/train.vi.txt\", # CẦN ĐẢM BẢO FILE NÀY TỒN TẠI\n        N_lines=10\n    )\n    print(\"Total Added segments:\", len(raw_ds_add), \"Dropped lines:\", dropped_add)\nexcept FileNotFoundError:\n    print(\"WARNING: train_vi_add.txt không được tìm thấy. Chỉ sử dụng dữ liệu gốc.\")\n    raw_ds_add = None\n\nif raw_ds_add:\n    train_ds = concatenate_datasets([train_ds_orig, raw_ds_add])\nelse:\n    train_ds = train_ds_orig\n\nprint(\"\\n--- FINAL DATASET SIZES ---\")\nprint(\"Train segments (Original + Added):\", len(train_ds))\nprint(\"Eval segments (Original only):\", len(eval_ds))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T09:54:26.524957Z","iopub.execute_input":"2025-12-17T09:54:26.525096Z","iopub.status.idle":"2025-12-17T09:54:37.073558Z","shell.execute_reply.started":"2025-12-17T09:54:26.525084Z","shell.execute_reply":"2025-12-17T09:54:37.073108Z"}},"outputs":[{"name":"stdout","text":"Total Original segments: 50000 Dropped lines: 0\nTotal Added segments: 50000 Dropped lines: 0\n\n--- FINAL DATASET SIZES ---\nTrain segments (Original + Added): 99500\nEval segments (Original only): 500\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom pathlib import Path\n\nMODEL_NAME = Path(\"/kaggle/working/Qwen3-1.7B-medical\")\n\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-1.7B\", use_fast=True)\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_NAME,\n    device_map=\"auto\",\n    dtype=torch.bfloat16,\n    attn_implementation=\"sdpa\",\n)\n\nsystem_prompt = (\n    \"You are a medical translation engine. Translate from English to Vietnamese. \"\n    \"Rules: Keep abbreviations as-is (e.g., V.A, V.a, PTA, Type B/C/As). \"\n    \"Preserve all numbers, %, ±, ≥, ≤,... parentheses, and punctuation. \"\n    \"Do not add explanations. Output only the Vietnamese translation.\"\n    \"Prioritize medical accuracy and use standard Vietnamese medical terminology.\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T09:54:37.074069Z","iopub.execute_input":"2025-12-17T09:54:37.074198Z","iopub.status.idle":"2025-12-17T09:54:38.335651Z","shell.execute_reply.started":"2025-12-17T09:54:37.074186Z","shell.execute_reply":"2025-12-17T09:54:38.335203Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"def to_prompt_completion(ex):\n    system_msg = {\"role\": \"system\", \"content\": system_prompt}\n    user_msg   = {\"role\": \"user\", \"content\": f\"Translate English to Vietnamese:\\n{ex['en']}\"}\n    asst_msg   = {\"role\": \"assistant\", \"content\": ex[\"vi\"]}\n\n    prompt_text = tokenizer.apply_chat_template(\n        [system_msg, user_msg],\n        tokenize=False,\n        add_generation_prompt=True,\n        enable_thinking=False,\n    )\n\n    full_text = tokenizer.apply_chat_template(\n        [system_msg, user_msg, asst_msg],\n        tokenize=False,\n        add_generation_prompt=False,\n        enable_thinking=False,\n    )\n\n    completion_text = full_text[len(prompt_text):]\n    return {\"prompt\": prompt_text, \"completion\": completion_text, \"en\": ex[\"en\"], \"vi\": ex[\"vi\"]}\n\ncolumns_to_remove = [col for col in train_ds.column_names if col in ['idx']]\n\ntrain_pc = train_ds.map(to_prompt_completion, remove_columns=columns_to_remove)\neval_pc  = eval_ds.map(to_prompt_completion,  remove_columns=columns_to_remove)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T09:54:38.336189Z","iopub.execute_input":"2025-12-17T09:54:38.336325Z","iopub.status.idle":"2025-12-17T09:55:09.048466Z","shell.execute_reply.started":"2025-12-17T09:54:38.336312Z","shell.execute_reply":"2025-12-17T09:55:09.048049Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/99500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"95f643c10019496d88d8523e77280130"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dafd124c2cb54785aa67491030a6eaa2"}},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"print(train_pc[0][\"prompt\"][:]) \nprint(train_pc[0][\"completion\"][:])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T09:55:09.048993Z","iopub.execute_input":"2025-12-17T09:55:09.049133Z","iopub.status.idle":"2025-12-17T09:55:09.054192Z","shell.execute_reply.started":"2025-12-17T09:55:09.049121Z","shell.execute_reply":"2025-12-17T09:55:09.053708Z"}},"outputs":[{"name":"stdout","text":"<|im_start|>system\nYou are a medical translation engine. Translate from English to Vietnamese. Rules: Keep abbreviations as-is (e.g., V.A, V.a, PTA, Type B/C/As). Preserve all numbers, %, ±, ≥, ≤,... parentheses, and punctuation. Do not add explanations. Output only the Vietnamese translation.Prioritize medical accuracy and use standard Vietnamese medical terminology.<|im_end|>\n<|im_start|>user\nTranslate English to Vietnamese:\nDiuretic-induced hyperuricemia without gout does not require treatment or discontinuation of the diuretic.\nDiuretics may slightly increase mortality in patients with a history of heart failure who do not have pulmonary congestion, particularly in those who are also taking an ACE inhibitor or angiotensin II receptor blocker and who do not drink at least 1400 mL (48 oz) of fluid daily.\nThe increased mortality is probably related to diuretic-induced hyponatremia and hypotension.\nAdrenergic modifiers Adrenergic modifiers include central alpha-2-agonists, postsynaptic alpha-1-blockers, and peripheral-acting non-selective adrenergic blockers (see table Adrenergic Modifiers for Hypertension).\nIn ECG gating, the image recording (or reconstruction) is synchronized with the electrocardiogram (ECG), providing information from several cardiac cycles that can be used to create single images of selected points in the cardiac cycle.\nCT gating uses the ECG to trigger the x-ray beam at the desired portion of the cardiac cycle, exposing the patient to less radiation than gating that simply reconstructs information from only the desired portion of the cardiac cycle (gated reconstruction) and does not interrupt the x-ray beam.\nChest x-rays Chest x-rays are often useful as a starting point in a cardiac diagnosis and should always be done when a diagnosis of heart failure is considered.\nPosteroanterior and lateral views provide a gross view of atrial and ventricular size and shape and pulmonary vasculature, but additional tests are almost always required for precise characterization of cardiac structure and function.\nCT Spiral (helical) CT may be used to evaluate pericarditis, congenital cardiac disorders (especially abnormal arteriovenous connections), disorders of the great vessels (eg, aortic aneurysm, aortic dissection), cardiac tumors, acute pulmonary embolism, chronic pulmonary thromboembolic disease, and arrhythmogenic right ventricular dysplasia.\nHowever, CT requires a radiopaque contrast agent, which may limit its use in patients with renal impairment.<|im_end|>\n<|im_start|>assistant\n<think>\n\n</think>\n\n\nTăng axit uric máu do dùng thuốc lợi tiểu mà không gây bệnh gout không cần phải điều trị hoặc ngưng dùng thuốc lợi tiểu.\nThuốc lợi tiểu có thể làm tăng nhẹ tỷ lệ tử vong ở những bệnh nhân có tiền sử suy tim không có ứ máu phổi, đặc biệt ở những người cũng đang dùng thuốc ức chế ACE hoặc thuốc chẹn thụ thể angiotensin II và những người không uống ít nhất 1400 mL nước (48 oz) mỗi ngày.\nTỷ lệ tử vong tăng lên có thể liên quan đến hạ natri máu và tụt huyết áp do thuốc lợi tiểu.\nThuốc cường adrenergic Các thuốc cường adrenergic bao gồm các thuốc chủ vận alpha-2 trung ương, các thuốc chẹn thụ thể alpha-1 sau synap và các thuốc chẹn adrenergic không chọn lọc ngoại vi (xem bảng Thuốc bổ sung Adrenergic cho Tăng huyết áp).\nTrong phương pháp chụp và tái tạo hình ảnh đồng bộ điện tâm đồ, điện tim được ghi đồng bộ với quá trình chụp để cho ta biết hình ảnh nào thì tương ứng với chu chuyển tim nào.\nĐồng bộ điện tâm đồ giúp cho bệnh nhân ít nhiễm bức xạ hơn so với phương pháp chụp đồng bộ điện tâm đồ thông thường trong đó tia X sẽ được phát ở toàn bộ chu chuyển tim của bệnh nhân, sau đó, ở giai đoạn tái tạo ảnh, kỹ thuật viên mới lựa chọn hình ảnh ở những chu chuyển tim nhất định dựa trên sóng điện tim được ghi đồng bộ trong quá trình chụp trước đó.\nChụp X-quang ngực Chụp X-quang ngực thường hữu ích là một điểm khởi đầu trong chẩn đoán tim và luôn phải được thực hiện khi xem xét chẩn đoán suy tim.\nPhim chụp tư thế sau-trước và tư thế nghiêng có thể cho ta cái nhìn toàn cảnh về hình dạng và kích thước tâm nhĩ, tâm thất và hệ mạch phổi. Tuy nhiên, để chẩn đoán chính xác cấu trúc và chức năng tim, ta luôn cần thêm các xét nghiệm khác.\nChụp CT Chụp cắt lớp xoắn ốc có thể được sử dụng để chẩn đoán viêm màng ngoài tim, bệnh tim bẩm sinh, đặc biệt là luồng thông động - tĩnh mạch thường, bệnh lý mạch máu lớn (ví dụ phình động mạch chủ, tách thành động mạch chủ, khối u tim, tắc mạch phổi cấp, huyết khối động mạch phổi mạn tính và bệnh loạn sản thất phải gây loạn nhịp.\nTuy nhiên, chụp cắt lớp đòi hỏi phải tiêm thuốc cản quang và do đó bị hạn chế ở bệnh nhân suy thận.<|im_end|>\n\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"from peft import LoraConfig\n\nlora_config = LoraConfig(\n    r=32,\n    lora_alpha=64,\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T09:55:09.054670Z","iopub.execute_input":"2025-12-17T09:55:09.054832Z","iopub.status.idle":"2025-12-17T09:55:09.065023Z","shell.execute_reply.started":"2025-12-17T09:55:09.054820Z","shell.execute_reply":"2025-12-17T09:55:09.064590Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"import math\nimport random\nimport torch\nimport wandb\nfrom transformers import TrainerCallback\nfrom trl import SFTTrainer, SFTConfig\nfrom sacrebleu.metrics import BLEU\n\nbleu_metric = BLEU(tokenize=\"none\", effective_order=True)\n\nclass PPLCallback(TrainerCallback):\n    def on_log(self, args, state, control, logs=None, **kwargs):\n        if not logs:\n            return\n        if \"loss\" in logs and logs[\"loss\"] is not None:\n            loss = float(logs[\"loss\"])\n            try:\n                logs[\"ppl\"] = math.exp(loss)\n            except OverflowError:\n                logs[\"ppl\"] = float(\"inf\")\n\n    def on_evaluate(self, args, state, control, metrics=None, **kwargs):\n        if not metrics:\n            return\n        if \"eval_loss\" in metrics and metrics[\"eval_loss\"] is not None:\n            loss = float(metrics[\"eval_loss\"])\n            try:\n                metrics[\"eval_ppl\"] = math.exp(loss)\n            except OverflowError:\n                metrics[\"eval_ppl\"] = float(\"inf\")\n\nclass BLEUCallback(TrainerCallback):\n    def __init__(self, tokenizer, eval_ds, system_prompt, n_samples=100):\n        self.tokenizer = tokenizer\n        self.eval_ds = eval_ds\n        self.system_prompt = system_prompt\n        self.n_samples = n_samples\n\n    def translate_one(self, model, en: str, max_new_tokens=2048):\n        messages = [\n            {\"role\": \"system\", \"content\": self.system_prompt},\n            {\"role\": \"user\", \"content\": f\"Translate English to Vietnamese:\\n{en}\"},\n        ]\n        prompt = self.tokenizer.apply_chat_template(\n            messages,\n            tokenize=False,\n            add_generation_prompt=True,\n            enable_thinking=False,\n        )\n        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n        \n        with torch.no_grad():\n            model.eval() \n            out = model.generate(\n                **inputs,\n                max_new_tokens=max_new_tokens,\n                temperature=0.2,\n                top_p=0.9,\n            )\n        return self.tokenizer.decode(out[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True).strip()\n    \n    def on_evaluate(self, args, state, control, metrics=None, **kwargs):\n        if state.is_local_process_zero:\n            model = kwargs['model']\n            data = random.sample(list(self.eval_ds), k=min(self.n_samples, len(self.eval_ds)))\n\n            refs = []\n            hyps = []\n            \n            unwrapped_model = model.module if hasattr(model, \"module\") else model\n\n            for ex in data:\n                pred = self.translate_one(unwrapped_model, ex[\"en\"])\n                hyps.append(pred)\n                refs.append(ex[\"vi\"])\n\n            # Tính BLEU score\n            score = bleu_metric.corpus_score(hyps, [refs]).score\n\n            # Log kết quả lên W&B\n            if args.report_to and \"wandb\" in args.report_to:\n                wandb.log({\n                    \"eval/bleu\": score,\n                    \"eval/bleu_n_samples\": len(data),\n                }, step=state.global_step)\n\n            if metrics is not None:\n                metrics[\"eval_bleu\"] = score\n            print(f\"\\nBLEU (tokenize=none) on {len(data)} samples: {score:.2f}\")\n\n        return control","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T09:55:09.066149Z","iopub.execute_input":"2025-12-17T09:55:09.066305Z","iopub.status.idle":"2025-12-17T09:55:09.274660Z","shell.execute_reply.started":"2025-12-17T09:55:09.066294Z","shell.execute_reply":"2025-12-17T09:55:09.274232Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"args = SFTConfig(\n    output_dir=\"Qwen3-1.7B-medical-LoRA\",\n    max_length=2048,\n    per_device_train_batch_size=16,\n    gradient_accumulation_steps=4,\n\n    gradient_checkpointing=True,\n    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n    \n    learning_rate=1e-4,\n    num_train_epochs=1,\n    warmup_ratio=0.03,\n    logging_steps=5,\n    eval_steps=100,\n    save_steps=100,\n    save_total_limit=4,\n    eval_strategy=\"steps\",\n    save_strategy=\"steps\",\n    \n\n    bf16=True,\n    completion_only_loss=True,\n    packing=False,\n    group_by_length=False,\n\n    report_to=[\"wandb\"],\n    run_name=run.name,\n)\n\nbleu_callback = BLEUCallback(\n    tokenizer=tokenizer,\n    eval_ds=eval_pc, \n    system_prompt=system_prompt,\n    n_samples=10,\n)\n\ntrainer = SFTTrainer(\n    model=model,\n    args=args,\n    train_dataset=train_pc,\n    eval_dataset=eval_pc,\n    peft_config=lora_config,\n    processing_class=tokenizer,\n    callbacks=[PPLCallback(), bleu_callback], \n)\n\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T09:55:09.275116Z","iopub.execute_input":"2025-12-17T09:55:09.275518Z","iopub.status.idle":"2025-12-17T13:43:58.201635Z","shell.execute_reply.started":"2025-12-17T09:55:09.275506Z","shell.execute_reply":"2025-12-17T13:43:58.201199Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Adding EOS to train dataset:   0%|          | 0/99500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6396028575764ada8635a001e10523f2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Tokenizing train dataset:   0%|          | 0/99500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d318314446064377975efb7e41603740"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Truncating train dataset:   0%|          | 0/99500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fe7960ae9d1d4b7985dc1da7533358bc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Adding EOS to eval dataset:   0%|          | 0/500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a51d170550649f9a8804fd2b375e6ae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Tokenizing eval dataset:   0%|          | 0/500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"271ab9bdfc2046ce9397220a1a104cee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Truncating eval dataset:   0%|          | 0/500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"59ff94edc98d48a290e8bb0780a9a77d"}},"metadata":{}},{"name":"stderr","text":"The model is already on multiple devices. Skipping the move to device specified in `args`.\nThe tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1555' max='1555' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1555/1555 3:43:45, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Entropy</th>\n      <th>Num Tokens</th>\n      <th>Mean Token Accuracy</th>\n      <th>Ppl</th>\n      <th>Bleu</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>0.883400</td>\n      <td>1.041244</td>\n      <td>1.834494</td>\n      <td>5670209.000000</td>\n      <td>0.766798</td>\n      <td>2.832739</td>\n      <td>21.094631</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.812400</td>\n      <td>0.943802</td>\n      <td>1.809879</td>\n      <td>11349846.000000</td>\n      <td>0.784024</td>\n      <td>2.569732</td>\n      <td>23.478552</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.760200</td>\n      <td>0.902939</td>\n      <td>1.761972</td>\n      <td>17021396.000000</td>\n      <td>0.792294</td>\n      <td>2.466842</td>\n      <td>24.671251</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.721600</td>\n      <td>0.874753</td>\n      <td>1.796619</td>\n      <td>22732438.000000</td>\n      <td>0.796803</td>\n      <td>2.398282</td>\n      <td>37.444362</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.713300</td>\n      <td>0.853596</td>\n      <td>1.782468</td>\n      <td>28412857.000000</td>\n      <td>0.800852</td>\n      <td>2.348075</td>\n      <td>43.513310</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.702300</td>\n      <td>0.840725</td>\n      <td>1.772480</td>\n      <td>34096277.000000</td>\n      <td>0.802966</td>\n      <td>2.318047</td>\n      <td>41.977407</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.660600</td>\n      <td>0.826913</td>\n      <td>1.793714</td>\n      <td>39789516.000000</td>\n      <td>0.804778</td>\n      <td>2.286250</td>\n      <td>19.755650</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.640100</td>\n      <td>0.818832</td>\n      <td>1.777114</td>\n      <td>45476834.000000</td>\n      <td>0.807018</td>\n      <td>2.267850</td>\n      <td>44.282744</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>0.670800</td>\n      <td>0.808511</td>\n      <td>1.798132</td>\n      <td>51156411.000000</td>\n      <td>0.808014</td>\n      <td>2.244562</td>\n      <td>39.913957</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.684500</td>\n      <td>0.801396</td>\n      <td>1.816726</td>\n      <td>56851859.000000</td>\n      <td>0.809115</td>\n      <td>2.228650</td>\n      <td>44.424218</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>0.656900</td>\n      <td>0.795445</td>\n      <td>1.835301</td>\n      <td>62536176.000000</td>\n      <td>0.809870</td>\n      <td>2.215427</td>\n      <td>30.183559</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>0.616400</td>\n      <td>0.789282</td>\n      <td>1.805836</td>\n      <td>68203786.000000</td>\n      <td>0.810899</td>\n      <td>2.201815</td>\n      <td>39.909199</td>\n    </tr>\n    <tr>\n      <td>1300</td>\n      <td>0.631600</td>\n      <td>0.787480</td>\n      <td>1.811722</td>\n      <td>73910852.000000</td>\n      <td>0.811553</td>\n      <td>2.197850</td>\n      <td>35.736005</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>0.664500</td>\n      <td>0.783941</td>\n      <td>1.820305</td>\n      <td>79580505.000000</td>\n      <td>0.812116</td>\n      <td>2.190086</td>\n      <td>41.618890</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.633600</td>\n      <td>0.782105</td>\n      <td>1.823243</td>\n      <td>85298811.000000</td>\n      <td>0.812520</td>\n      <td>2.186069</td>\n      <td>38.840800</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"\nBLEU (tokenize=none) on 10 samples: 21.09\n\nBLEU (tokenize=none) on 10 samples: 23.48\n\nBLEU (tokenize=none) on 10 samples: 24.67\n\nBLEU (tokenize=none) on 10 samples: 37.44\n\nBLEU (tokenize=none) on 10 samples: 43.51\n\nBLEU (tokenize=none) on 10 samples: 41.98\n\nBLEU (tokenize=none) on 10 samples: 19.76\n\nBLEU (tokenize=none) on 10 samples: 44.28\n\nBLEU (tokenize=none) on 10 samples: 39.91\n\nBLEU (tokenize=none) on 10 samples: 44.42\n\nBLEU (tokenize=none) on 10 samples: 30.18\n\nBLEU (tokenize=none) on 10 samples: 39.91\n\nBLEU (tokenize=none) on 10 samples: 35.74\n\nBLEU (tokenize=none) on 10 samples: 41.62\n\nBLEU (tokenize=none) on 10 samples: 38.84\n","output_type":"stream"},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1555, training_loss=0.7185183158641459, metrics={'train_runtime': 13433.6373, 'train_samples_per_second': 7.407, 'train_steps_per_second': 0.116, 'total_flos': 1.145411217321984e+18, 'train_loss': 0.7185183158641459, 'epoch': 1.0})"},"metadata":{}}],"execution_count":11}]}