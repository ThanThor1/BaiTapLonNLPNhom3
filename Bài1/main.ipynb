{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-12-23T12:23:50.543236Z",
     "iopub.status.busy": "2025-12-23T12:23:50.542691Z",
     "iopub.status.idle": "2025-12-23T12:23:54.462811Z",
     "shell.execute_reply": "2025-12-23T12:23:54.462300Z",
     "shell.execute_reply.started": "2025-12-23T12:23:50.543218Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sacrebleu in /usr/local/lib/python3.12/dist-packages (2.5.1)\n",
      "Requirement already satisfied: portalocker in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (3.2.0)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (2025.11.3)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (0.9.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (2.0.2)\n",
      "Requirement already satisfied: colorama in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (0.4.6)\n",
      "Requirement already satisfied: lxml in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (5.4.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!pip install sacrebleu\n",
    "!pip -q install wandb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-23T12:23:54.464087Z",
     "iopub.status.busy": "2025-12-23T12:23:54.463937Z",
     "iopub.status.idle": "2025-12-23T12:23:54.469297Z",
     "shell.execute_reply": "2025-12-23T12:23:54.468903Z",
     "shell.execute_reply.started": "2025-12-23T12:23:54.464069Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports & Setup Done.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import html\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.cuda.amp import GradScaler\n",
    "from tqdm.notebook import tqdm\n",
    "from datasets import Dataset as HfDataset, DatasetDict\n",
    "from sacrebleu.metrics import BLEU\n",
    "\n",
    "# Tokenizers\n",
    "from tokenizers import (\n",
    "    Tokenizer, models, trainers, pre_tokenizers, \n",
    "    normalizers, decoders, processors\n",
    ")\n",
    "\n",
    "# Thêm đường dẫn code (trên kaggle)\n",
    "CODE_DIR = \"/kaggle/input/dataset1/Transformer/\"\n",
    "if CODE_DIR not in sys.path:\n",
    "    sys.path.append(CODE_DIR)\n",
    "\n",
    "# --- REPRODUCIBILITY ---\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-23T12:23:54.470058Z",
     "iopub.status.busy": "2025-12-23T12:23:54.469923Z",
     "iopub.status.idle": "2025-12-23T12:23:54.483210Z",
     "shell.execute_reply": "2025-12-23T12:23:54.482832Z",
     "shell.execute_reply.started": "2025-12-23T12:23:54.470044Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config Loaded. Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Tệp cấu hình chi tiết cho mô hình Transformer: bao gồm thiết lập thiết bị,tham số dữ liệu, siêu tham số huấn luyện, kiến trúc mô hình và nơi lưu trữ trên kaggle.\n",
    "\n",
    "CFG = {\n",
    "    \"seed\": 42,\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    \"use_cuda\": torch.cuda.is_available(),\n",
    "    \n",
    "    \"data_dir\": \"data\",\n",
    "    \"src_lang\": \"en\",\n",
    "    \"tgt_lang\": \"vi\",\n",
    "    \"vocab_size\": 16000,\n",
    "    \"max_len\": 128,\n",
    "    \"num_workers\": 4,\n",
    "    \n",
    "    \"batch_size\": 128,\n",
    "    \"accumulate_steps\": 2,\n",
    "    \"epochs\": 25,\n",
    "    \"base_lr\": 1.0,\n",
    "    \"warmup_steps\": 800,\n",
    "    \"lr_factor\": 1.0,\n",
    "    \"label_smoothing\": 0.1,\n",
    "    \"grad_clip\": 1.0,\n",
    "    \"mixed_precision\": \"bf16\", \n",
    "    \n",
    "    \"d_model\": 512, \n",
    "    \"num_heads\": 8, \n",
    "    \"num_layers\": 6, \n",
    "    \"d_ff\": 2048, \n",
    "    \"dropout\": 0.2, \n",
    "    \"tie_weights\": True,\n",
    "    \n",
    "    \"beam_size\": 4, \n",
    "    \"max_decode_len\": 60, \n",
    "    \"length_penalty_alpha\": 0.8,\n",
    "    \"bleu_tokenize\": \"13a\",   \n",
    "    \"bleu_lowercase\": False,   \n",
    "    \n",
    "    \"ckpt_dir\": \"./checkpoints\",\n",
    "    \"avg_loss_window\": 10,\n",
    "    \"avg_topk_loss_k\": 5,\n",
    "    \"avg_topk_loss_name\": \"avg_top5_best_loss.pth\",\n",
    "    \"wandb_log_every\": 10,\n",
    "    \"val_every\": 200\n",
    "}\n",
    "\n",
    "CFG[\"avg_path\"] = os.path.join(CFG[\"ckpt_dir\"], CFG[\"avg_topk_loss_name\"])\n",
    "CFG[\"window_dir\"] = os.path.join(CFG[\"ckpt_dir\"], \"window_checkpoints\")\n",
    "\n",
    "DATA_DIR = \"/kaggle/input/dataset3/Transformer/data\"\n",
    "FILE_PATHS = {\n",
    "    \"train\": {CFG[\"src_lang\"]: os.path.join(DATA_DIR, f\"train.{CFG['src_lang']}\"), \n",
    "              CFG[\"tgt_lang\"]: os.path.join(DATA_DIR, f\"train.{CFG['tgt_lang']}\")},\n",
    "    \"validation\": {CFG[\"src_lang\"]: os.path.join(DATA_DIR, f\"validation.{CFG['src_lang']}\"),\n",
    "                   CFG[\"tgt_lang\"]: os.path.join(DATA_DIR, f\"validation.{CFG['tgt_lang']}\")},\n",
    "    \"test\": {CFG[\"src_lang\"]: os.path.join(DATA_DIR, f\"test.{CFG['src_lang']}\"),\n",
    "             CFG[\"tgt_lang\"]: os.path.join(DATA_DIR, f\"test.{CFG['tgt_lang']}\")},\n",
    "}\n",
    "\n",
    "seed_everything(CFG[\"seed\"])\n",
    "os.makedirs(CFG[\"ckpt_dir\"], exist_ok=True)\n",
    "os.makedirs(CFG[\"window_dir\"], exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-23T12:23:54.483897Z",
     "iopub.status.busy": "2025-12-23T12:23:54.483770Z",
     "iopub.status.idle": "2025-12-23T12:23:54.493250Z",
     "shell.execute_reply": "2025-12-23T12:23:54.492885Z",
     "shell.execute_reply.started": "2025-12-23T12:23:54.483883Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Utils Defined.\n"
     ]
    }
   ],
   "source": [
    "# Các hàm tiện ích xử lý dữ liệu: nạp và làm sạch văn bản từ tệp nguồn, đồng bộ hóa cặp câu song ngữ và huấn luyện bộ tách từ BPE Tokenizer.\n",
    "\n",
    "def load_and_clean_data(file_paths, src_lang, tgt_lang):\n",
    "    raw_data = {}\n",
    "    for split, paths in file_paths.items():\n",
    "        with open(paths[src_lang], \"r\", encoding=\"utf-8\", errors=\"replace\") as f_src, \\\n",
    "             open(paths[tgt_lang], \"r\", encoding=\"utf-8\", errors=\"replace\") as f_tgt:\n",
    "            src_lines = [html.unescape(line.strip()) for line in f_src]\n",
    "            tgt_lines = [html.unescape(line.strip()) for line in f_tgt]\n",
    "        \n",
    "        min_len = min(len(src_lines), len(tgt_lines))\n",
    "        data = [{\"translation\": {src_lang: s, tgt_lang: t}} for s, t in zip(src_lines[:min_len], tgt_lines[:min_len])]\n",
    "        raw_data[split] = HfDataset.from_list(data)\n",
    "        print(f\"-> {split}: {len(data)} lines loaded.\")\n",
    "        \n",
    "    return DatasetDict(raw_data)\n",
    "\n",
    "def train_tokenizer(dataset, lang, vocab_size):\n",
    "    tok = Tokenizer(models.BPE(unk_token=\"[UNK]\"))\n",
    "    tok.normalizer = normalizers.Sequence([normalizers.NFKC(), normalizers.StripAccents()]) if lang == \"en\" else normalizers.Sequence([normalizers.NFC()])\n",
    "    tok.pre_tokenizer = pre_tokenizers.Sequence([pre_tokenizers.Metaspace(), pre_tokenizers.Punctuation()])\n",
    "    \n",
    "    trainer = trainers.BpeTrainer(vocab_size=vocab_size, special_tokens=[\"[PAD]\", \"[UNK]\", \"[BOS]\", \"[EOS]\", \"[MASK]\"], min_frequency=2)\n",
    "    tok.train_from_iterator((item[\"translation\"][lang] for item in dataset), trainer)\n",
    "    \n",
    "    tok.post_processor = processors.TemplateProcessing(\n",
    "        single=\"[BOS] $A [EOS]\",\n",
    "        special_tokens=[(\"[BOS]\", tok.token_to_id(\"[BOS]\")), (\"[EOS]\", tok.token_to_id(\"[EOS]\"))]\n",
    "    )\n",
    "    tok.decoder = decoders.Metaspace()\n",
    "    return tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-23T12:23:54.494373Z",
     "iopub.status.busy": "2025-12-23T12:23:54.494239Z",
     "iopub.status.idle": "2025-12-23T12:24:01.480181Z",
     "shell.execute_reply": "2025-12-23T12:24:01.479797Z",
     "shell.execute_reply.started": "2025-12-23T12:23:54.494361Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data...\n",
      "-> train: 133166 lines loaded.\n",
      "-> validation: 1553 lines loaded.\n",
      "-> test: 1268 lines loaded.\n",
      "Training Tokenizers...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Data Ready. Src Vocab: 16000, Tgt Vocab: 16000\n"
     ]
    }
   ],
   "source": [
    "# 1. Load Data\n",
    "print(\"Loading Data...\")\n",
    "datasets = load_and_clean_data(FILE_PATHS, CFG[\"src_lang\"], CFG[\"tgt_lang\"])\n",
    "\n",
    "# 2. Train Tokenizers\n",
    "print(\"Training Tokenizers...\")\n",
    "tokenizer_src = train_tokenizer(datasets[\"train\"], CFG[\"src_lang\"], CFG[\"vocab_size\"])\n",
    "tokenizer_tgt = train_tokenizer(datasets[\"train\"], CFG[\"tgt_lang\"], CFG[\"vocab_size\"])\n",
    "\n",
    "# 3. Global Constants\n",
    "PAD_ID = tokenizer_src.token_to_id(\"[PAD]\")\n",
    "BOS_ID = tokenizer_tgt.token_to_id(\"[BOS]\")\n",
    "EOS_ID = tokenizer_tgt.token_to_id(\"[EOS]\")\n",
    "SRC_VOCAB_SIZE = tokenizer_src.get_vocab_size()\n",
    "TGT_VOCAB_SIZE = tokenizer_tgt.get_vocab_size()\n",
    "\n",
    "print(f\"Data Ready. Src Vocab: {SRC_VOCAB_SIZE}, Tgt Vocab: {TGT_VOCAB_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-23T12:26:03.963086Z",
     "iopub.status.busy": "2025-12-23T12:26:03.962866Z",
     "iopub.status.idle": "2025-12-23T12:26:03.969492Z",
     "shell.execute_reply": "2025-12-23T12:26:03.969098Z",
     "shell.execute_reply.started": "2025-12-23T12:26:03.963071Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaders Created.\n"
     ]
    }
   ],
   "source": [
    "# Định nghĩa lớp CleanCollate để xử lý batch và hàm get_loaders để khởi tạo các DataLoader.\n",
    "\n",
    "class CleanCollate:\n",
    "    def __init__(self, tok_src, tok_tgt, cfg):\n",
    "        self.tok_src = tok_src\n",
    "        self.tok_tgt = tok_tgt\n",
    "        self.src_lang = cfg[\"src_lang\"]\n",
    "        self.tgt_lang = cfg[\"tgt_lang\"]\n",
    "        self.max_len = cfg[\"max_len\"]\n",
    "        self.pad_id = tok_src.token_to_id(\"[PAD]\")\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        src_batch, tgt_batch = [], []\n",
    "        for item in batch:\n",
    "            s_txt = item[\"translation\"][self.src_lang]\n",
    "            t_txt = item[\"translation\"][self.tgt_lang]\n",
    "            \n",
    "            s_ids = self.tok_src.encode(str(s_txt)).ids[:self.max_len]\n",
    "            t_ids = self.tok_tgt.encode(str(t_txt)).ids[:self.max_len]\n",
    "            \n",
    "            src_batch.append(torch.tensor(s_ids, dtype=torch.long))\n",
    "            tgt_batch.append(torch.tensor(t_ids, dtype=torch.long))\n",
    "            \n",
    "        src_padded = pad_sequence(src_batch, batch_first=True, padding_value=self.pad_id)\n",
    "        tgt_padded = pad_sequence(tgt_batch, batch_first=True, padding_value=self.pad_id)\n",
    "        return src_padded, tgt_padded\n",
    "\n",
    "def get_loaders(datasets, cfg):\n",
    "    collate_fn = CleanCollate(tokenizer_src, tokenizer_tgt, cfg)\n",
    "    \n",
    "    train_loader = DataLoader(datasets[\"train\"], batch_size=cfg[\"batch_size\"], shuffle=True, collate_fn=collate_fn, num_workers=cfg[\"num_workers\"], pin_memory=True, persistent_workers=True)\n",
    "    val_loader = DataLoader(datasets[\"validation\"], batch_size=cfg[\"batch_size\"], shuffle=False, collate_fn=collate_fn)\n",
    "    test_loader = DataLoader(datasets[\"test\"], batch_size=cfg[\"batch_size\"], shuffle=False, collate_fn=collate_fn)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "train_loader, val_loader, test_loader = get_loaders(datasets, CFG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-23T12:26:07.294966Z",
     "iopub.status.busy": "2025-12-23T12:26:07.294727Z",
     "iopub.status.idle": "2025-12-23T12:26:13.472507Z",
     "shell.execute_reply": "2025-12-23T12:26:13.472016Z",
     "shell.execute_reply.started": "2025-12-23T12:26:07.294948Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch Compile Activated!\n",
      "Model & Optimizer Setup Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_106/325268236.py:25: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG[\"use_cuda\"])\n"
     ]
    }
   ],
   "source": [
    "from model.Transformer import Transformer \n",
    "\n",
    "# 1. Model Init\n",
    "model = Transformer(\n",
    "    src_vocab_size=SRC_VOCAB_SIZE, tgt_vocab_size=TGT_VOCAB_SIZE,\n",
    "    d_model=CFG[\"d_model\"], num_heads=CFG[\"num_heads\"], d_ff=CFG[\"d_ff\"],\n",
    "    num_encoder_layers=CFG[\"num_layers\"], num_decoder_layers=CFG[\"num_layers\"],\n",
    "    dropout=CFG[\"dropout\"], pad_id=PAD_ID, tie_weights=CFG[\"tie_weights\"]\n",
    ").to(CFG[\"device\"])\n",
    "\n",
    "# Xavier Init\n",
    "for p in model.parameters():\n",
    "    if p.dim() > 1: nn.init.xavier_uniform_(p)\n",
    "\n",
    "# Torch Compile (Optimization)\n",
    "try:\n",
    "    model = torch.compile(model, mode=\"default\")\n",
    "    print(\"Torch Compile Activated!\")\n",
    "except:\n",
    "    print(\"Torch Compile Failed, using eager mode.\")\n",
    "\n",
    "# 2. Optimizer & Loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=CFG[\"base_lr\"], betas=(0.9, 0.98), eps=1e-9)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_ID, label_smoothing=CFG[\"label_smoothing\"])\n",
    "scaler = GradScaler(enabled=CFG[\"use_cuda\"])\n",
    "\n",
    "# 3. Scheduler (Noam)\n",
    "def rate(step, model_size, factor, warmup):\n",
    "    if step == 0: step = 1\n",
    "    return factor * (model_size ** (-0.5) * min(step ** (-0.5), step * warmup ** (-1.5)))\n",
    "\n",
    "lr_scheduler = optim.lr_scheduler.LambdaLR(\n",
    "    optimizer, lr_lambda=lambda step: rate(step, CFG[\"d_model\"], CFG[\"lr_factor\"], CFG[\"warmup_steps\"])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-23T12:26:18.500849Z",
     "iopub.status.busy": "2025-12-23T12:26:18.500148Z",
     "iopub.status.idle": "2025-12-23T12:26:18.513396Z",
     "shell.execute_reply": "2025-12-23T12:26:18.512934Z",
     "shell.execute_reply.started": "2025-12-23T12:26:18.500828Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Metric Utils Ready.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sacrebleu.metrics import BLEU\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "\n",
    "_SENT_END_RE = re.compile(r'(^|[.!?]\\s+)([\"“”‘’\\(\\[\\{]*)([A-Za-zÀ-ỹ])')\n",
    "\n",
    "def _normalize_text(s: str) -> str:\n",
    "    # Làm sạch text, xử lý unicode và artifact BPE\n",
    "    s = unicodedata.normalize(\"NFC\", s)\n",
    "    s = s.replace(\"▁\", \" \")\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def _detok_punct(s: str) -> str:\n",
    "    # Gắn liền dấu câu vào từ đứng trước\n",
    "    s = re.sub(r\"\\s+([.,!?;:%)\\]\\}])\", r\"\\1\", s)\n",
    "    s = re.sub(r\"([(\\[\\{])\\s+\", r\"\\1\", s)\n",
    "    return s\n",
    "\n",
    "def _trim_hyp_ids(ids):\n",
    "    # Cắt bỏ token thừa trong output model\n",
    "    if len(ids) > 0 and ids[0] == BOS_ID:\n",
    "        ids = ids[1:]\n",
    "    if EOS_ID in ids:\n",
    "        ids = ids[:ids.index(EOS_ID)]\n",
    "    ids = [x for x in ids if x != PAD_ID]\n",
    "    return ids\n",
    "\n",
    "\n",
    "def beam_search_decode(model, src, src_mask, max_len, beam_size):\n",
    "    memory = model.encoder(src, src_mask)\n",
    "    memory = memory.repeat(beam_size, 1, 1)\n",
    "    src_mask = src_mask.repeat(beam_size, 1, 1, 1)\n",
    "    beam = [(0.0, [BOS_ID])]\n",
    "    \n",
    "    for _ in range(max_len):\n",
    "        candidates = []\n",
    "        beam_inputs = pad_sequence([torch.tensor(b[1], device=CFG[\"device\"]) for b in beam], \n",
    "                                   batch_first=True, padding_value=PAD_ID)\n",
    "        tgt_mask = model.make_tgt_mask(beam_inputs)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits = model.decoder(beam_inputs, memory[:len(beam)], tgt_mask, src_mask[:len(beam)])[:, -1, :]\n",
    "            log_probs = F.log_softmax(logits, dim=-1)\n",
    "            \n",
    "        for i, (score, seq) in enumerate(beam):\n",
    "            if seq[-1] == EOS_ID: \n",
    "                candidates.append((score, seq)); continue\n",
    "            \n",
    "            vals, idxs = log_probs[i].topk(beam_size)\n",
    "            for v, idx in zip(vals, idxs):\n",
    "                candidates.append((score + v.item(), seq + [idx.item()]))\n",
    "                \n",
    "        beam = sorted(candidates, key=lambda x: x[0] / (len(x[1])**CFG[\"length_penalty_alpha\"]), reverse=True)[:beam_size]\n",
    "        if all(b[1][-1] == EOS_ID for b in beam): break\n",
    "        \n",
    "    return beam[0][1]\n",
    "\n",
    "# Bleu caculation\n",
    "bleu_metric = BLEU(tokenize=CFG[\"bleu_tokenize\"], lowercase=CFG[\"bleu_lowercase\"])\n",
    "\n",
    "def calculate_bleu(model, loader, limit=None):\n",
    "    model.eval()\n",
    "    hyps, refs = [], []\n",
    "    ds = loader.dataset\n",
    "    \n",
    "    # Logic chọn index rải đều\n",
    "    n_total = len(ds)\n",
    "    n_eval = min(limit, n_total) if limit else n_total\n",
    "    step = max(1, n_total // n_eval)\n",
    "    idxs = list(range(0, n_total, step))[:n_eval]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(idxs, desc=\"Calculating BLEU\", leave=False):\n",
    "            item = ds[int(i)]\n",
    "            src_text = item[\"translation\"][CFG[\"src_lang\"]]\n",
    "            ref_text = item[\"translation\"][CFG[\"tgt_lang\"]]\n",
    "            \n",
    "            # Encode\n",
    "            src_ids = torch.tensor(tokenizer_src.encode(src_text).ids).unsqueeze(0).to(CFG[\"device\"])\n",
    "            src_mask = (src_ids != PAD_ID).unsqueeze(1).unsqueeze(2)\n",
    "            \n",
    "            # Decode\n",
    "            out_ids = beam_search_decode(model, src_ids, src_mask, CFG[\"max_decode_len\"], CFG[\"beam_size\"])\n",
    "            out_ids = _trim_hyp_ids(out_ids)\n",
    "            hyp_text = tokenizer_tgt.decode(out_ids, skip_special_tokens=True)\n",
    "            \n",
    "            hyp_clean = _detok_punct(_normalize_text(hyp_text))\n",
    "            hyps.append(hyp_clean)\n",
    "           \n",
    "            ref_clean = _detok_punct(_normalize_text(ref_text))\n",
    "            refs.append(ref_clean)\n",
    "            \n",
    "    return bleu_metric.corpus_score(hyps, [refs]).score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-12-23T12:24:01.489866Z",
     "iopub.status.idle": "2025-12-23T12:24:01.490006Z",
     "shell.execute_reply": "2025-12-23T12:24:01.489944Z",
     "shell.execute_reply.started": "2025-12-23T12:24:01.489936Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from kaggle_secrets import UserSecretsClient\n",
    "import wandb\n",
    "\n",
    "wandb_key = UserSecretsClient().get_secret(\"WANDB_API_KEY\")\n",
    "wandb.login(key=wandb_key)\n",
    "\n",
    "wandb.init(\n",
    "    project=\"lossvalid\",\n",
    "    config=CFG,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-12-23T12:24:01.490967Z",
     "iopub.status.idle": "2025-12-23T12:24:01.491180Z",
     "shell.execute_reply": "2025-12-23T12:24:01.491103Z",
     "shell.execute_reply.started": "2025-12-23T12:24:01.491092Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "wandb.run.name = f\"lr{CFG.get('lr','')}_bs{CFG.get('batch_size','')}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-12-23T12:24:01.491688Z",
     "iopub.status.idle": "2025-12-23T12:24:01.491830Z",
     "shell.execute_reply": "2025-12-23T12:24:01.491773Z",
     "shell.execute_reply.started": "2025-12-23T12:24:01.491765Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "global_step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-12-23T12:24:01.492322Z",
     "iopub.status.idle": "2025-12-23T12:24:01.492448Z",
     "shell.execute_reply": "2025-12-23T12:24:01.492394Z",
     "shell.execute_reply.started": "2025-12-23T12:24:01.492386Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, optimizer, scheduler, scaler):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    dtype = torch.bfloat16 if CFG[\"mixed_precision\"] == \"bf16\" else torch.float16\n",
    "\n",
    "    pbar = tqdm(train_loader, desc=\"Training\", leave=False)\n",
    "    for i, (src, tgt) in enumerate(pbar):\n",
    "        src, tgt = src.to(CFG[\"device\"]), tgt.to(CFG[\"device\"])\n",
    "        tgt_in, tgt_out = tgt[:, :-1], tgt[:, 1:]\n",
    "\n",
    "        with torch.amp.autocast(device_type=\"cuda\", dtype=dtype, enabled=CFG[\"use_cuda\"]):\n",
    "            logits = model(src, tgt_in)\n",
    "            loss = criterion(logits.reshape(-1, TGT_VOCAB_SIZE), tgt_out.reshape(-1))\n",
    "            loss = loss / CFG[\"accumulate_steps\"]\n",
    "\n",
    "        if CFG[\"mixed_precision\"] == \"bf16\":\n",
    "            loss.backward()\n",
    "            if (i + 1) % CFG[\"accumulate_steps\"] == 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), CFG[\"grad_clip\"])\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "        else:\n",
    "            scaler.scale(loss).backward()\n",
    "            if (i + 1) % CFG[\"accumulate_steps\"] == 0:\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), CFG[\"grad_clip\"])\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "        loss_unscaled = loss.item() * CFG[\"accumulate_steps\"]\n",
    "        total_loss += loss_unscaled\n",
    "        pbar.set_postfix(loss=f\"{total_loss/(i+1):.4f}\")\n",
    "\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "def validate(model, loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for src, tgt in loader:\n",
    "            src, tgt = src.to(CFG[\"device\"]), tgt.to(CFG[\"device\"])\n",
    "            with torch.amp.autocast(device_type=\"cuda\", enabled=CFG[\"use_cuda\"]):\n",
    "                logits = model(src, tgt[:, :-1])\n",
    "                loss = criterion(logits.reshape(-1, TGT_VOCAB_SIZE), tgt[:, 1:].reshape(-1))\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "def average_checkpoints(paths):\n",
    "    # Cộng trung bình trọng số các file model\n",
    "    print(f\"Averaging {len(paths)} checkpoints...\")\n",
    "    avg_state = {}\n",
    "    states = [torch.load(p, map_location=\"cpu\") for p in paths]\n",
    "\n",
    "    states = [s[\"model_state_dict\"] if \"model_state_dict\" in s else s for s in states]\n",
    "    \n",
    "    for k in states[0].keys():\n",
    "        params = [s[k] for s in states]\n",
    "        if torch.is_floating_point(params[0]):\n",
    "            avg_state[k] = sum(params) / len(params)\n",
    "        else:\n",
    "            avg_state[k] = params[0] # Keep Long/Int params as is\n",
    "            \n",
    "    return avg_state\n",
    "\n",
    "print(\"Training Pipeline Defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-12-23T12:24:01.492956Z",
     "iopub.status.idle": "2025-12-23T12:24:01.493075Z",
     "shell.execute_reply": "2025-12-23T12:24:01.493023Z",
     "shell.execute_reply.started": "2025-12-23T12:24:01.493016Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import os\n",
    "\n",
    "window_ckpts = [] \n",
    "\n",
    "print(\"START TRAINING (Per-Epoch Logging)...\")\n",
    "print(f\"{'EPOCH':<6} | {'TRAIN LOSS':<12} | {'VAL LOSS':<12} |\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "for epoch in range(CFG[\"epochs\"]):\n",
    "    # Chạy 1 epoch train và lấy loss trung bình\n",
    "    train_loss_avg = train_epoch(model, train_loader, optimizer, lr_scheduler, scaler)\n",
    "    \n",
    "    # Chạy validate và lấy loss trung bình\n",
    "    val_loss_avg = validate(model, val_loader)\n",
    "    \n",
    "    # Log lên WandB sau khi kết thúc 1 epoch\n",
    "    if wandb.run is not None:\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"train_loss\": train_loss_avg,\n",
    "            \"val_loss\": val_loss_avg,\n",
    "            \"lr\": optimizer.param_groups[0]['lr']\n",
    "        })\n",
    "    \n",
    "    # In màn hình\n",
    "    print(f\"{epoch+1:<6} | {train_loss_avg:<12.4f} | {val_loss_avg:<12.4f} |\")\n",
    "    \n",
    "    # Lưu checkpoint\n",
    "    if epoch >= (CFG[\"epochs\"] - CFG[\"avg_loss_window\"]):\n",
    "        path = os.path.join(CFG[\"window_dir\"], f\"ep{epoch+1}_loss{val_loss_avg:.4f}.pth\")\n",
    "        torch.save(model.state_dict(), path)\n",
    "        window_ckpts.append((val_loss_avg, path))\n",
    "\n",
    "print(\"\\nHoàn thành huấn luyện.\")\n",
    "\n",
    "# Sắp xếp các checkpoint theo val_loss thấp nhất\n",
    "window_ckpts.sort(key=lambda x: x[0])\n",
    "\n",
    "# Lấy Top-K checkpoint tốt nhất\n",
    "top_k_paths = [p for _, p in window_ckpts[:CFG[\"avg_topk_loss_k\"]]]\n",
    "\n",
    "if top_k_paths:\n",
    "    avg_weights = average_checkpoints(top_k_paths)\n",
    "    torch.save(avg_weights, CFG[\"avg_path\"])\n",
    "    print(f\"Lưu Averaged Model (Top-{len(top_k_paths)} Best Loss) tới: {CFG['avg_path']}\")\n",
    "else:\n",
    "    print(\"error.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-12-23T12:24:01.493569Z",
     "iopub.status.idle": "2025-12-23T12:24:01.493702Z",
     "shell.execute_reply": "2025-12-23T12:24:01.493648Z",
     "shell.execute_reply.started": "2025-12-23T12:24:01.493630Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-12-23T12:24:01.494190Z",
     "iopub.status.idle": "2025-12-23T12:24:01.494330Z",
     "shell.execute_reply": "2025-12-23T12:24:01.494271Z",
     "shell.execute_reply.started": "2025-12-23T12:24:01.494263Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "if test_loader is not None:\n",
    "    target_loader = test_loader\n",
    "    split_name = \"FULL TEST\"\n",
    "else:\n",
    "    target_loader = val_loader\n",
    "    split_name = \"FULL VALIDATION (Thay thế)\"\n",
    "\n",
    "output_txt_file = \"model_outputs.txt\"\n",
    "\n",
    "print(f\" MỤC TIÊU: Chấm điểm & Xuất file bản dịch trên tập {split_name}\\n\")\n",
    "\n",
    "ckpt_path = CFG.get(\"avg_path\", \"path_not_found\")\n",
    "\n",
    "if os.path.exists(ckpt_path):\n",
    "    print(f\" Đang xử lý: MODEL AVERAGED\")\n",
    "    \n",
    "    try:\n",
    "        # 1. Load trọng số\n",
    "        state_dict = torch.load(ckpt_path, map_location=CFG[\"device\"])\n",
    "        if isinstance(state_dict, dict) and (\"model_state_dict\" in state_dict):\n",
    "            state_dict = state_dict[\"model_state_dict\"]\n",
    "\n",
    "        model.load_state_dict(state_dict)\n",
    "        model.eval()\n",
    "        print(f\"   Đã tải checkpoint: {os.path.basename(ckpt_path)}\")\n",
    "        \n",
    "        # 2. Dịch toàn bộ tập dữ liệu\n",
    "        hyps, refs = [], []\n",
    "        ds = target_loader.dataset\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i in tqdm(range(len(ds)), desc=\"Translating\"):\n",
    "                item = ds[i]\n",
    "                src_text = item[\"translation\"][CFG[\"src_lang\"]]\n",
    "                ref_text = item[\"translation\"][CFG[\"tgt_lang\"]]\n",
    "                \n",
    "                # Encode & Decode\n",
    "                src_ids = torch.tensor(tokenizer_src.encode(src_text).ids).unsqueeze(0).to(CFG[\"device\"])\n",
    "                src_mask = (src_ids != PAD_ID).unsqueeze(1).unsqueeze(2)\n",
    "                \n",
    "                # Sử dụng Beam Search đã định nghĩa ở Cell 8\n",
    "                out_ids = beam_search_decode(model, src_ids, src_mask, CFG[\"max_decode_len\"], CFG[\"beam_size\"])\n",
    "                out_ids = _trim_hyp_ids(out_ids)\n",
    "                hyp_text = tokenizer_tgt.decode(out_ids, skip_special_tokens=True)\n",
    "                \n",
    "                # Làm sạch text theo đúng logic post-processing\n",
    "                hyp_clean = _detok_punct(_normalize_text(hyp_text))\n",
    "                ref_clean = _detok_punct(_normalize_text(ref_text))\n",
    "                \n",
    "                hyps.append(hyp_clean)\n",
    "                refs.append(ref_clean)\n",
    "\n",
    "        # 3. Tính điểm BLEU để hiển thị\n",
    "        score = bleu_metric.corpus_score(hyps, [refs]).score\n",
    "        print(f\"\\n   KẾT QUẢ {split_name}:\")\n",
    "        print(f\"      + BLEU: {score:.2f}\")\n",
    "\n",
    "        # 4. XUẤT FILE CHỈ CHỨA BẢN DỊCH (Mỗi câu một dòng)\n",
    "        print(f\"   Đang xuất bản dịch ra file: {output_txt_file}\")\n",
    "        with open(output_txt_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            for line in hyps:\n",
    "                f.write(line + \"\\n\")\n",
    "        \n",
    "        print(f\"   Hoàn tất! File '{output_txt_file}' đã sẵn sàng trong phần Output.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   Lỗi: {e}\")\n",
    "\n",
    "else:\n",
    "    print(f\" Không tìm thấy checkpoint tại: {ckpt_path}\")\n",
    "\n",
    "print(\"\\n Xong!\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaH100",
   "dataSources": [
    {
     "databundleVersionId": 14559231,
     "sourceId": 118448,
     "sourceType": "competition"
    },
    {
     "datasetId": 9083595,
     "sourceId": 14237758,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 9096868,
     "sourceId": 14256776,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 9104237,
     "sourceId": 14266966,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 9105317,
     "sourceId": 14268399,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31236,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
